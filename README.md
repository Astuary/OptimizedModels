●	Implemented an optimal centring of the inputs by augmenting the logistic regression with regularization model with a vector of centring parameters  

●	Combatted the problem of outliers corrupting the standard least-squares linear regression learning, by Huber regression loss functions that are less sensitive to more substantial errors

●	Computed the sparsest subgradient descent for support vector classifier (SVC) model (with the most weights set to 0) whose test error rate was not statistically significantly worse than the model with the lowest test error rate 

●	Orchestrated a custom neural network model that simultaneously solves a regression problem and a multi-class classification problem

●	Dealt with the case of missing feature values by imputations during learning and prediction with the Laplacian mixture models, by calculating the class conditional mean
